{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datatable as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = '../input/riiid-test-answer-prediction/'\n",
    "MY_DATA_PATH = './my_data/'\n",
    "CACHE_PATH = './lgb1215weights/'\n",
    "\n",
    "DATA_PATH = '/home/zuoyuhui/datasets/riid准确回答/'\n",
    "file_train = 'train.csv'\n",
    "file_questions = 'questions.csv'\n",
    "file_lectures = 'lectures.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(CACHE_PATH):\n",
    "    os.mkdir(CACHE_PATH)\n",
    "    \n",
    "    \n",
    "DEBUG = True\n",
    "OFFLINE = True\n",
    "CV5 = False #联系cv5\n",
    "CV = False #普通cut\n",
    "\n",
    "if OFFLINE:\n",
    "    nrows = 1250000\n",
    "else:\n",
    "    nrows = None\n",
    "\n",
    "if DEBUG:\n",
    "    MY_DATA_PATH = f'{MY_DATA_PATH}/debug/'\n",
    "    CACHE_PATH = f'{CACHE_PATH}/debug/'\n",
    "    if not os.path.exists(CACHE_PATH):\n",
    "        os.mkdir(CACHE_PATH)\n",
    "        \n",
    "config_file = f'{CACHE_PATH}/config.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(dic,save_path):\n",
    "    with gzip.open(save_path,'wb') as f:\n",
    "        pickle.dump(dic,f)\n",
    "        \n",
    "def load_pickle(load_path):\n",
    "    with gzip.open(load_path,'rb') as f:\n",
    "        message_dict = pickle.load(f)\n",
    "    return message_dict\n",
    "\n",
    "#定义内存压缩方法\n",
    "def reduce_mem_usage(df,verbose=True):\n",
    "    start_mem = df.memory_usage().sum()/ 1024**2\n",
    "    numerics = ['int16','int32','int64','float16','float32','float64']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                #NumPy分别提供numpy.iinfo 并numpy.finfo 验证NumPy整数和浮点值的最小值或最大值：\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float64).min and c_max < np.finfo(np.float64).max:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "            end_men = df.memory_usage().sum()/1024**2\n",
    "    print('Memory usage after optimization is :{:.2f} MB'.format(end_men))\n",
    "    print('Decreased by {:1f}%'.format(100*(start_mem - end_men)/start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================================================\n",
    "\n",
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_data\n",
    "questions_df = pd.read_csv(f'{DATA_PATH}/questions.csv')\n",
    "questions_df['content_bundle_same'] = (questions_df['question_id'] == questions_df['bundle_id']).astype(int)  # 取出question_id 和 bundle_id：解决问题的代码。类别变量 相同的数据 9765个\n",
    "\n",
    "questions_df['tags_len'] = questions_df['tags'].apply(lambda x:0 if str(x)=='nan' else len(str(x).split(' '))) # 统计tags的个数 nan记为0\n",
    "\n",
    "questions_df['part_content_num'] = questions_df.groupby('part')['question_id'].transform('count') # 问题所属part的数量\n",
    "\n",
    "questions_df['tags'] = questions_df['tags'].apply(lambda x: [] if str(x) == 'nan' else str(x).split(' ')) # 切分tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_bundle_dict = dict(zip(questions_df.question_id.values,questions_df.bundle_id.values))\n",
    "question_part_dict = dict(zip(questions_df.question_id.values, questions_df.part.values))\n",
    "question_tags_dict = dict(zip(questions_df.question_id.values, questions_df.tags.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bundle_id\n",
       "1400        [1400, 1401, 1402]\n",
       "1403        [1403, 1404, 1405]\n",
       "1406        [1406, 1407, 1408]\n",
       "1409        [1409, 1410, 1411]\n",
       "1412        [1412, 1413, 1414]\n",
       "                 ...          \n",
       "13238    [13238, 13239, 13240]\n",
       "13241    [13241, 13242, 13243]\n",
       "13244    [13244, 13245, 13246]\n",
       "13247    [13247, 13248, 13249]\n",
       "13250    [13250, 13251, 13252]\n",
       "Name: question_id, Length: 1614, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bundle_df = questions_df.groupby('bundle_id')['question_id'].unique() # 相当于把list变成set 返回问题类别的所有唯一值\n",
    "bundle_df = bundle_df[bundle_df.apply(len)>1]  \n",
    "bundle_df # 找出同类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_mapping ={}\n",
    "for id_list in bundle_df.values:\n",
    "    bid = id_list[0]\n",
    "    for qid in id_list:\n",
    "        bundle_mapping[qid] = (bid,len(id_list))  #1400: (1400, 3) 找出一个类别有几个个数大于1的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------\n",
    "## lecture_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lectures_df = pd.read_csv(f'{DATA_PATH}/lectures.csv')\n",
    "lecture_tag_dict = dict(zip(lectures_df.lecture_id.values, lectures_df.tag.values))  # 演讲的标签代码\n",
    "lecture_part_dict = dict(zip(lectures_df.lecture_id.values, lectures_df.part.values)) #  讲座的顶级类别代码\n",
    "lecture_type_dict = dict(zip(lectures_df.lecture_id.values, lectures_df.type_of.values)) # 简要介绍讲座的核心目的 解决问题类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lecture_id</th>\n",
       "      <th>tag</th>\n",
       "      <th>part</th>\n",
       "      <th>type_of</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "      <td>159</td>\n",
       "      <td>5</td>\n",
       "      <td>concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192</td>\n",
       "      <td>79</td>\n",
       "      <td>5</td>\n",
       "      <td>solving question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>317</td>\n",
       "      <td>156</td>\n",
       "      <td>5</td>\n",
       "      <td>solving question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>32535</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>solving question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>32570</td>\n",
       "      <td>113</td>\n",
       "      <td>3</td>\n",
       "      <td>solving question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>32604</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>32625</td>\n",
       "      <td>142</td>\n",
       "      <td>2</td>\n",
       "      <td>concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>32736</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>concept</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lecture_id  tag  part           type_of\n",
       "0            89  159     5           concept\n",
       "1           100   70     1           concept\n",
       "2           185   45     6           concept\n",
       "3           192   79     5  solving question\n",
       "4           317  156     5  solving question\n",
       "..          ...  ...   ...               ...\n",
       "413       32535    8     5  solving question\n",
       "414       32570  113     3  solving question\n",
       "415       32604   24     6           concept\n",
       "416       32625  142     2           concept\n",
       "417       32736   82     3           concept\n",
       "\n",
       "[418 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lectures_df.groupby('lecture_id').tail(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------\n",
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 14.8 µs\n",
      "Train|Valid: 98730332|2500000\n",
      "Memory usage after optimization is :646.33 MB\n",
      "Decreased by 41.666662%\n",
      "Memory usage after optimization is :16.38 MB\n",
      "Decreased by 41.666486%\n",
      "Memory usage after optimization is :2636.38 MB\n",
      "Decreased by 26.315789%\n",
      "Memory usage after optimization is :66.76 MB\n",
      "Decreased by 26.315753%\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "if OFFLINE:\n",
    "    feld_needed = ['row_id', 'timestamp', 'user_id', 'content_id', 'content_type_id', 'task_container_id',\n",
    "                   'user_answer', 'answered_correctly', 'prior_question_elapsed_time', 'prior_question_had_explanation']\n",
    "    \n",
    "    if CV5:\n",
    "#             val_size = 250000\n",
    "        val_size = 2500000\n",
    "        train = dt.fread(DATA_PATH+file_train, max_nrows=None,columns=feld_needed).to_pandas()\n",
    "        valid_split1 = train.groupby('user_id').tail(5)  \n",
    "        train_split1 = train[~train.row_id.isin(valid_split1.row_id)]\n",
    "        valid_split1 = valid_split1[valid_split1.content_type_id == 0]\n",
    "        train_split1 = train_split1[train_split1.content_type_id == 0]\n",
    "        print(f'{train_split1.answered_correctly.mean():.3f} {valid_split1.answered_correctly.mean():.3f}')\n",
    "        \n",
    "        max_timestamp_u = train[['user_id','timestamp']].groupby(['user_id']).agg(['max']).reset_index()  # 得到每个用户最大的时间戳\n",
    "        max_timestamp_u.columns = ['user_id', 'max_time_stamp'] \n",
    "        MAX_TIME_STAMP = max_timestamp_u.max_time_stamp.max()  # 所有用户最大时间  因为是时间戳\n",
    "        # (MAX_TIME_STAMP for all users) - (max_time_stamp for each user)  所有用户的最大时间减去一个用户的最大时间就是这个用户开始的时间戳\n",
    "        def rand_time(max_time_stamp):\n",
    "            interval = MAX_TIME_STAMP - max_time_stamp\n",
    "            rand_time_stamp = random.randint(0,interval)\n",
    "            return rand_time_stamp\n",
    "        # 由于训练数据和测试数据是按时间拆分的，因此验证数据也应该按时间拆分。但是，给定的时间戳是自用户的第一个事件以来经过的时间，而不是实际时间。因此，我在一定间隔内为每个用户设置了随机的首次访问时间。\n",
    "        max_timestamp_u['rand_time_stamp'] = max_timestamp_u.max_time_stamp.apply(rand_time)\n",
    "        train = train.merge(max_timestamp_u, on='user_id', how='left')\n",
    "        train['viretual_time_stamp'] = train.timestamp + train['rand_time_stamp']\n",
    "        \n",
    "        train = train.sort_values(['viretual_time_stamp', 'row_id']).reset_index(drop=True)\n",
    "        #现在我们已经按viretual_time_amp对数据帧进行了排序，我们可以轻松地按时间拆分数据帧。\n",
    "        \n",
    "        for cv in range(5):\n",
    "            valid = train[-val_size:]\n",
    "            train = train[:-val_size]\n",
    "            # check new users and new contents\n",
    "            new_users = len(valid[~valid.user_id.isin(train.user_id)].user_id.unique())\n",
    "            valid_question = valid[valid.content_type_id == 0]\n",
    "            train_question = train[train.content_type_id == 0]\n",
    "            new_contents = len(valid_question[~valid_question.content_id.isin(train_question.content_id)].content_id.unique())    \n",
    "            print(f'cv{cv} {train_question.answered_correctly.mean():.3f} {valid_question.answered_correctly.mean():.3f} {new_users} {new_contents}')\n",
    "            valid.to_pickle(f'cv{cv+1}_valid.pickle')\n",
    "            train.to_pickle(f'cv{cv+1}_train.pickle')\n",
    "            \n",
    "    train = pd.read_pickle(f'./cv1_train.pickle')[feld_needed]\n",
    "    valid = pd.read_pickle(f'./cv1_valid.pickle')[feld_needed]\n",
    "    print(f'Train|Valid: {len(train)}|{len(valid)}')\n",
    "    \n",
    "    '''\n",
    "    Make feat for valid:\n",
    "    * ques: quest_train\n",
    "    * lect: lect_train\n",
    "\n",
    "    Make feat for test:\n",
    "    * ques: quest_train/quest_valid\n",
    "    * lect: lect_train/lect_valid\n",
    "    ''' \n",
    "    \n",
    "    # 训练集验证集都取出 问题的数据的这三列\n",
    "    ques_train = train.loc[train.content_type_id == False, ['row_id','content_id','answered_correctly']].reset_index(drop=True)\n",
    "    ques_valid = valid.loc[valid.content_type_id == False, ['row_id','content_id','answered_correctly']].reset_index(drop=True) \n",
    "    # 对他们进行量化\n",
    "    ques_train = reduce_mem_usage(ques_train, verbose=True)\n",
    "    ques_valid = reduce_mem_usage(ques_valid, verbose=True)\n",
    "    # 对验证集和训练集 的 用户在回答上一个问题包(忽略其间的任何讲座)后是否看到了解释和正确的回答 做填充\n",
    "    train['prior_question_had_explanation'] = train['prior_question_had_explanation'].fillna(False).astype('int8') \n",
    "    valid['prior_question_had_explanation'] = valid['prior_question_had_explanation'].fillna(False).astype('int8')\n",
    "    \n",
    "    # 对用户对问题包答题的nan进行均值填充\n",
    "    prior_question_elapsed_time_mean = train.loc[train.content_type_id == False].prior_question_elapsed_time.dropna().values.mean()\n",
    "    train['prior_question_elapsed_time'] = train['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_mean).astype(np.int32)\n",
    "    valid['prior_question_elapsed_time'] = valid['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_mean).astype(np.int32)\n",
    "    train = reduce_mem_usage(train, verbose=True)\n",
    "    valid = reduce_mem_usage(valid, verbose=True)\n",
    "    \n",
    "    save_pickle(prior_question_elapsed_time_mean, f'./prior_question_elapsed_time_mean.pkl')\n",
    "    config_dict = {}\n",
    "else:\n",
    "    config_dict = load_pickle(config_file)\n",
    "    prior_question_elapsed_time_mean = load_pickle(f'{CACHE_PATH}/prior_question_elapsed_time_mean.pkl')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13523"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------\n",
    "## Content static Feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_content_feat(df, type):\n",
    "    df = df.loc[df.content_type_id==False].reset_index(drop=True)\n",
    "    file_name = f'content_feat_{type}.pkl'\n",
    "    \n",
    "    # 每个题目的全局平均准确率\n",
    "    feat_df = df.groupby('content_id', as_index=False)['answered_correctly'].mean().rename(columns={'answered_correctly':'content_target_mean'})\n",
    "    # 每个题目全局出现次数\n",
    "    content_cnt = df.groupby('content_id')['user_id'].count()\n",
    "    feat_df['content_cnt'] = content_cnt.reindex(feat_df.content_id.values).values\n",
    "    \n",
    "    save_pickle(feat_df, save_path=f'./{file_name}')\n",
    "    feat_df = reduce_mem_usage(feat_df, verbose=True)\n",
    "    return feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is :0.21 MB\n",
      "Decreased by 50.000000%\n",
      "Memory usage after optimization is :0.21 MB\n",
      "Decreased by 50.000000%\n",
      "content_feat:\n",
      "    content_id  content_target_mean  content_cnt\n",
      "0           0             0.907227         6777\n",
      "1           1             0.890625         7265\n",
      "2           2             0.554199        43781\n",
      "3           3             0.779297        22451\n",
      "4           4             0.613770        30881\n"
     ]
    }
   ],
   "source": [
    "if OFFLINE:\n",
    "    content_feat = make_content_feat(df=train.copy(deep=True),type='train')\n",
    "    content_feat_test = make_content_feat(df=pd.concat([train,valid]),type='test')\n",
    "    print('content_feat:\\n',content_feat.head())\n",
    "else:\n",
    "    content_feat_test = load_pickle(f'./content_feat_test.pkl')\n",
    "    content_feat_test = reduce_mem_usage(content_feat_test, verbose=True)\n",
    "    \n",
    "content_target_mean_dict = dict(zip(content_feat_test.content_id.values,\n",
    "                                    content_feat_test.content_target_mean.values))\n",
    "content_feat_cols = [col for col in content_feat_test if col != 'content_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------\n",
    "## Part Fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_part_mean_dict(df):\n",
    "    df = df.loc[df.content_type_id==False].reset_index(drop=True)\n",
    "    df = df.merge(questions_df[['question_id','part']], left_on='content_id', right_on='question_id',how='left')\n",
    "    \n",
    "    feat_df = df.groupby('part', as_index=False)['answered_correctly'].mean(). rename(columns={'answered_correctly': 'part_target_mean'})\n",
    "    return dict(zip(feat_df.part.values,feat_df.part_target_mean.values))\n",
    "\n",
    "if OFFLINE:\n",
    "    part_target_mean_dict = make_part_mean_dict(df=pd.concat([train[['content_id', 'content_type_id', 'answered_correctly']],\n",
    "                                                              valid[['content_id', 'content_type_id', 'answered_correctly']]]))\n",
    "    save_pickle(part_target_mean_dict, f'{CACHE_PATH}/part_target_mean_dict.pkl')\n",
    "else:\n",
    "    part_target_mean_dict = load_pickle(f'./part_target_mean_dict.pkl')\n",
    "    \n",
    "questions_df['part_target_mean'] = questions_df['part'].apply(lambda x: part_target_mean_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------\n",
    "## Id static feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is :1200.32 MB\n",
      "Decreased by 35.000000%\n",
      "Memory usage after optimization is :30.42 MB\n",
      "Decreased by 35.000000%\n"
     ]
    }
   ],
   "source": [
    "static_feat_cols = ['part','prior_question_elapsed_time']\n",
    "\n",
    "def get_stat_feat(df,feat_cols):\n",
    "    df = df.loc[df.content_type_id==False].reset_index(drop=True)\n",
    "    df = df.merge(questions_df[['question_id','part']],left_on='content_id',right_on='question_id',how='left')\n",
    "    return df[feat_cols]\n",
    "\n",
    "if OFFLINE:\n",
    "    state_feat_train = get_stat_feat(df=train.copy(deep=True), feat_cols=static_feat_cols)\n",
    "    state_feat_valid = get_stat_feat(df=valid.copy(deep=True), feat_cols=static_feat_cols)\n",
    "    state_feat_train = reduce_mem_usage(state_feat_train, verbose=True)\n",
    "    state_feat_valid = reduce_mem_usage(state_feat_valid, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96817414 2453886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98730332"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(state_feat_train),len(state_feat_valid))\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------\n",
    "## User Loop Feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 25\n",
    "if OFFLINE:\n",
    "    user_cnt_dict = defaultdict(int) # 当字典里的key不存在但被查找时，返回的不是keyError而是一个默认值\n",
    "    user_pos_cnt_dict = defaultdict(int) # 比如list对应[ ]，str对应的是空字符串，set对应set( )，int对应0\n",
    "    user_part_cnt_dict = defaultdict(int)\n",
    "    user_part_pos_cnt_dict = defaultdict(int)\n",
    "    user_content_cnt_dict = defaultdict(int)\n",
    "    user_content_pos_cnt_dict = defaultdict(int)\n",
    "    user_content_redo_cnt_dict = defaultdict(int)\n",
    "    user_content_mean_sum_dict = defaultdict(int)\n",
    "    user_consecutive_pos_cnt_dict = defaultdict(int)\n",
    "    user_target_win25_dict = defaultdict(list)\n",
    "    user_content_mean_win10_dict = defaultdict(list)\n",
    "\n",
    "    user_explanation_cnt_dict = defaultdict(int)\n",
    "    user_explanation_pos_cnt_dict = defaultdict(int)\n",
    "    user_elapse_time_sum_dict = defaultdict(int)\n",
    "    user_elapse_time_win10_dict = defaultdict(list)\n",
    "    user_last_timestamp_dict = defaultdict(int)\n",
    "    user_last_task_dict = defaultdict(int)\n",
    "    user_content_win5_dict = defaultdict(list)\n",
    "    user_part_win10_dict = defaultdict(list)\n",
    "\n",
    "    bundle_state_dict = defaultdict(list) # bundle_id, time_diff\n",
    "    # user_order_in_session_dict = defaultdict(int)\n",
    "    user_cum_time_dict = defaultdict(int)\n",
    "    user_timespan_win10_dict = defaultdict(list)\n",
    "\n",
    "    user_tags_cnt_dict = defaultdict(int)\n",
    "    user_tags_pos_cnt_dict = defaultdict(int)\n",
    "\n",
    "    user_continue_quest_cnt_dict = defaultdict(int)\n",
    "else:\n",
    "    user_content_feat_df = pd.read_pickle(f'{CACHE_PATH}/user_content_feat.pkl')\n",
    "    user_content_feat_df = reduce_mem_usage(user_content_feat_df)\n",
    "    user_content_cnt_dict = defaultdict(int)\n",
    "    user_content_pos_cnt_dict = defaultdict(int)\n",
    "\n",
    "    user_cnt_dict = load_pickle(f'{CACHE_PATH}/user_cnt_dict.pkl')\n",
    "    user_pos_cnt_dict = load_pickle(f'{CACHE_PATH}/user_pos_cnt_dict.pkl')\n",
    "    user_part_cnt_dict = load_pickle(f'{CACHE_PATH}/user_part_cnt_dict.pkl')\n",
    "    user_part_pos_cnt_dict = load_pickle(f'{CACHE_PATH}/user_part_pos_cnt_dict.pkl')\n",
    "    # user_content_cnt_dict = load_pickle(f'{CACHE_PATH}/user_content_cnt_dict.pkl')\n",
    "    user_content_redo_cnt_dict = load_pickle(f'{CACHE_PATH}/user_content_redo_cnt_dict.pkl')\n",
    "    user_content_mean_sum_dict = load_pickle(f'{CACHE_PATH}/user_content_mean_sum_dict.pkl')\n",
    "    user_consecutive_pos_cnt_dict = load_pickle(f'{CACHE_PATH}/user_consecutive_pos_cnt_dict.pkl')\n",
    "    user_target_win25_dict = load_pickle(f'{CACHE_PATH}/user_target_win25_dict.pkl')\n",
    "    user_content_mean_win10_dict = load_pickle(f'{CACHE_PATH}/user_content_mean_win10_dict.pkl')\n",
    "\n",
    "    user_explanation_cnt_dict = load_pickle(f'{CACHE_PATH}/user_explanation_cnt_dict.pkl')\n",
    "    user_explanation_pos_cnt_dict = load_pickle(f'{CACHE_PATH}/user_explanation_pos_cnt_dict.pkl')\n",
    "    user_elapse_time_sum_dict = load_pickle(f'{CACHE_PATH}/user_elapse_time_sum_dict.pkl')\n",
    "    user_elapse_time_win10_dict = load_pickle(f'{CACHE_PATH}/user_elapse_time_win10_dict.pkl')\n",
    "    user_last_timestamp_dict = load_pickle(f'{CACHE_PATH}/user_last_timestamp_dict.pkl')\n",
    "    user_last_task_dict = load_pickle(f'{CACHE_PATH}/user_last_task_dict.pkl')\n",
    "    user_content_win5_dict = load_pickle(f'{CACHE_PATH}/user_content_win5_dict.pkl')\n",
    "    user_part_win10_dict = load_pickle(f'{CACHE_PATH}/user_part_win10_dict.pkl')\n",
    "\n",
    "    bundle_state_dict = load_pickle(f'{CACHE_PATH}/bundle_state_dict.pkl')\n",
    "    # user_order_in_session_dict = load_pickle(f'{CACHE_PATH}/user_order_in_session_dict.pkl')\n",
    "    user_cum_time_dict = load_pickle(f'{CACHE_PATH}/user_cum_time_dict.pkl')\n",
    "    user_timespan_win10_dict = load_pickle(f'{CACHE_PATH}/user_timespan_win10_dict.pkl')\n",
    "\n",
    "    user_tags_cnt_dict = load_pickle(f'{CACHE_PATH}/user_tags_cnt_dict.pkl')\n",
    "    user_tags_pos_cnt_dict = load_pickle(f'{CACHE_PATH}/user_tags_pos_cnt_dict.pkl')\n",
    "\n",
    "    user_continue_quest_cnt_dict = load_pickle(f'{CACHE_PATH}/user_continue_quest_cnt_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = ['user_id', 'content_id', 'task_container_id', 'answered_correctly', 'prior_question_elapsed_time',\n",
    "             'prior_question_had_explanation', 'content_type_id', 'timestamp']\n",
    "\n",
    "def make_user_loop_feature(df, content_target_mean_dict,\n",
    "                          user_cnt_dict, \n",
    "                          user_pos_cnt_dict,\n",
    "                          user_part_cnt_dict,\n",
    "                          user_part_pos_cnt_dict,\n",
    "                          user_content_cnt_dict,\n",
    "                          user_content_pos_cnt_dict,\n",
    "                          user_content_redo_cnt_dict,\n",
    "                          user_content_mean_sum_dict,\n",
    "                          user_consecutive_pos_cnt_dict, \n",
    "                          user_target_win25_dict, \n",
    "                          user_content_mean_win10_dict,\n",
    "                          user_explanation_cnt_dict,\n",
    "                          user_explanation_pos_cnt_dict, \n",
    "                          user_elapse_time_sum_dict,\n",
    "                          user_elapse_time_win10_dict,\n",
    "                          user_last_timestamp_dict,\n",
    "                          user_last_task_dict,\n",
    "                          user_content_win5_dict,\n",
    "                          user_part_win10_dict,\n",
    "                          bundle_state_dict,\n",
    "                          user_cum_time_dict, \n",
    "                          user_timespan_win10_dict,# user_order_in_session_dict,\n",
    "                          user_tags_cnt_dict,\n",
    "                          user_tags_pos_cnt_dict,\n",
    "                          user_continue_quest_cnt_dict,\n",
    "                          update=True, isTrain=True):\n",
    "    sample_num = len(df.loc[df.content_type_id == False])\n",
    "\n",
    "    user_cnt_npy = np.zeros(sample_num)\n",
    "    user_pos_cnt_npy = np.zeros(sample_num)\n",
    "    user_part_cnt_npy = np.zeros(sample_num)\n",
    "    user_part_pos_cnt_npy = np.zeros(sample_num)\n",
    "    user_content_cnt_npy = np.zeros(sample_num)\n",
    "    user_content_pos_cnt_npy = np.zeros(sample_num)\n",
    "    user_content_redo_cnt_npy = np.zeros(sample_num)\n",
    "    user_content_mean_mean_npy = np.zeros(sample_num)\n",
    "    user_consecutive_pos_cnt_npy = np.zeros(sample_num)\n",
    "    user_pos_cnt_win25_npy = np.zeros(sample_num)\n",
    "    user_content_mean_win10_npy = np.zeros(sample_num)\n",
    "\n",
    "    user_explanation_cnt_npy = np.zeros(sample_num)\n",
    "    user_explanation_pos_cnt_npy = np.zeros(sample_num)\n",
    "    user_elapse_time_mean_npy = np.zeros(sample_num)\n",
    "    user_elapse_time_mean_win10_npy = np.zeros(sample_num)\n",
    "    user_last_timespan_npy = np.zeros(sample_num)\n",
    "    user_last_task_diff_npy = np.zeros(sample_num)\n",
    "    user_content_appear_in_win5_npy = np.zeros(sample_num)\n",
    "    user_part_cnt_in_win10_npy = np.zeros(sample_num)\n",
    "\n",
    "    # user_order_in_session_npy = np.zeros(sample_num)\n",
    "    user_cum_time_npy = np.zeros(sample_num)\n",
    "    user_timespan_win10_mean_npy = np.zeros(sample_num)\n",
    "\n",
    "    user_tags_cnt_mean_npy = np.zeros(sample_num)\n",
    "    user_tags_pos_rate_npy = np.zeros(sample_num)\n",
    "\n",
    "    user_continue_quest_cnt_npy = np.zeros(sample_num)\n",
    "\n",
    "    if update:\n",
    "        tk0 = tqdm(df[used_cols].values)\n",
    "    else:\n",
    "        tk0 = df[used_cols].values\n",
    "    idx = 0\n",
    "    for(_user_id,_content_id,_task_container_id, _answered_correctly, _prior_question_elapsed_time,\n",
    "           _prior_question_had_explanation, _content_type_id, _timestamp) in tk0:\n",
    "        \n",
    "        if _content_id in content_target_mean_dict:\n",
    "            _content_target_mean = content_target_mean_dict[_content_id]\n",
    "        else:\n",
    "            _content_target_mean = 0\n",
    "            \n",
    "        if _content_type_id == False:\n",
    "            _bundle_id = question_bundle_dict[_content_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
