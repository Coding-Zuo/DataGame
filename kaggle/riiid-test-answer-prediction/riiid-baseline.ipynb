{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Riiid Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "# import optuna.integration.lightgbm as lgb\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import riiideducation\n",
    "\n",
    "try:\n",
    "    env = riiideducation.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrows = 10_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "# train = pd.read_hdf('../input/riiid-train-data-multiple-formats/riiid_train.h5', stop=nrows)\n",
    "\n",
    "question_contents = pd.read_pickle('../input/riiid-preprocess-data/user_content.pkl')\n",
    "questions = pd.read_pickle('../input/riiid-preprocess-data/questions.pkl')\n",
    "\n",
    "with open('../input/riiid-preprocess-data/user_id_idxs.pkl', 'rb') as f:\n",
    "    user_id_idxs = pickle.load(f)\n",
    "\n",
    "print(f'{time() - start:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split to feature-generating data and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_train_val_idxs(user_id_idxs, feature_size, train_size, val_size, new_user_frac=.2):\n",
    "    feature_idxs, train_idxs, val_idxs = [], [], []\n",
    "    np.random.seed(42)\n",
    "\n",
    "    for indices in random.sample(list(user_id_idxs), len(user_id_idxs)):\n",
    "        if len(feature_idxs) > feature_size:\n",
    "            break\n",
    "\n",
    "        if len(val_idxs) < val_size:\n",
    "            if np.random.rand() < new_user_frac:\n",
    "                val_idxs.extend(indices)\n",
    "            else:\n",
    "                offset = np.random.randint(len(indices)//2, len(indices))\n",
    "                feature_idxs.extend(indices[:len(indices)//2])\n",
    "                train_idxs.extend(indices[len(indices)//2:offset])\n",
    "                val_idxs.extend(indices[offset:])\n",
    "        else:\n",
    "            if len(train_idxs) < train_size:\n",
    "                feature_idxs.extend(indices[:len(indices)//2])\n",
    "                train_idxs.extend(indices[len(indices)//2:])\n",
    "            else:\n",
    "                feature_idxs.extend(indices)\n",
    "    return feature_idxs, train_idxs, val_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = 50_000_000\n",
    "train_size = 30_000_000\n",
    "val_size = 2_500_000\n",
    "\n",
    "start = time()\n",
    "feature_idxs, train_idxs, val_idxs = get_feature_train_val_idxs(user_id_idxs, \n",
    "                                                                feature_size, \n",
    "                                                                train_size, \n",
    "                                                                val_size, \n",
    "                                                                new_user_frac=.2)\n",
    "\n",
    "print(len(feature_idxs), len(train_idxs), len(val_idxs))\n",
    "print(f'{time() - start:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = question_contents.loc[feature_idxs]\n",
    "train_df = question_contents.loc[train_idxs]\n",
    "val_df = question_contents.loc[val_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del question_contents, feature_idxs, train_idxs, val_idxs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [\"answered_correctly\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_state(feature_df):\n",
    "    users_state = defaultdict(lambda:{\n",
    "        'user_accuracy':0.660, \n",
    "        'correctly_answered_content_cnt':0, \n",
    "        'answered_content_cnt':0, \n",
    "        'user_content_attempts':defaultdict(lambda:0)\n",
    "    })\n",
    "\n",
    "    for user_id, content_id, answer in feature_df[['user_id', 'content_id', 'answered_correctly']].values:\n",
    "        if users_state[user_id][\"user_content_attempts\"][content_id] < 5:\n",
    "            users_state[user_id][\"user_content_attempts\"][content_id] += 1\n",
    "\n",
    "        users_state[user_id][\"correctly_answered_content_cnt\"] += answer\n",
    "        users_state[user_id][\"answered_content_cnt\"] += 1\n",
    "\n",
    "        if users_state[user_id][\"answered_content_cnt\"] >= 1:\n",
    "            users_state[user_id][\"user_accuracy\"] = users_state[user_id][\"correctly_answered_content_cnt\"] \\\n",
    "            / users_state[user_id][\"answered_content_cnt\"]\n",
    "    \n",
    "    return users_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "users_state = get_users_state(feature_df)\n",
    "print(f'{time() - start:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del feature_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update users_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_users_state(users_state, prev_test_df):\n",
    "    for user_id, content_id, answer in prev_test_df[['user_id', 'content_id', 'answered_correctly']].values:\n",
    "        if users_state[user_id][\"user_content_attempts\"][content_id] < 5:\n",
    "            users_state[user_id][\"user_content_attempts\"][content_id] += 1\n",
    "\n",
    "        users_state[user_id][\"correctly_answered_content_cnt\"] += answer\n",
    "        users_state[user_id][\"answered_content_cnt\"] += 1\n",
    "\n",
    "        if users_state[user_id][\"answered_content_cnt\"] >= 1:\n",
    "            users_state[user_id][\"user_accuracy\"] = users_state[user_id][\"correctly_answered_content_cnt\"] / users_state[user_id][\"answered_content_cnt\"]\n",
    "\n",
    "    return users_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data(data, users_state, questions):\n",
    "    start = time()\n",
    "    \n",
    "    user_accuracy = []\n",
    "    answered_content_cnt = []\n",
    "    correctly_answered_content_cnt = []\n",
    "    user_content_attempts = []\n",
    "    \n",
    "    data = data.copy()\n",
    "    \n",
    "    for user_id, content_id in tqdm(data[['user_id', 'content_id']].values):\n",
    "        user_accuracy.append(users_state[user_id]['user_accuracy'])\n",
    "        answered_content_cnt.append(users_state[user_id]['answered_content_cnt'])\n",
    "        correctly_answered_content_cnt.append(users_state[user_id]['correctly_answered_content_cnt'])\n",
    "        user_content_attempts.append(min(5, users_state[user_id]['user_content_attempts'][content_id] + 1))\n",
    "    \n",
    "    data['user_accuracy'] = user_accuracy\n",
    "    data['answered_content_cnt'] = answered_content_cnt\n",
    "    data['correctly_answered_content_cnt'] = correctly_answered_content_cnt\n",
    "    data['user_content_attempts'] = user_content_attempts\n",
    "    \n",
    "    data = data.merge(questions, how='left', on='content_id')\n",
    "    \n",
    "    data['hmean_user_content_accuracy'] = 2 * (data['user_accuracy'] * data['content_accuracy']) / (data['user_accuracy'] + data['content_accuracy'])\n",
    "    data['hmean_user_part_accuracy'] = 2 * (data['user_accuracy'] * data['part_accuracy']) / (data['user_accuracy'] + data['part_accuracy'])\n",
    "    data['hmean_user_tags_accuracy'] = 2 * (data['user_accuracy'] * data['tags_accuracy']) / (data['user_accuracy'] + data['tags_accuracy'])\n",
    "    \n",
    "    data['prior_question_elapsed_time'].fillna(23916, inplace=True)\n",
    "#     data['prior_question_had_explanation'].fillna(False, inplace=True)\n",
    "    \n",
    "    print(f'{time() - start:.2f}')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_train_df = update_data(train_df, users_state, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_state = update_users_state(users_state, updated_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_val_df = update_data(val_df, users_state, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df, val_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    # user-based features\n",
    "    \"user_accuracy\",\n",
    "    \"correctly_answered_content_cnt\",\n",
    "    \"answered_content_cnt\",\n",
    "    \n",
    "    # content-based features\n",
    "    \"content_accuracy\",\n",
    "#     'tags_accuracy',\n",
    "#     'part_accuracy',\n",
    "    \n",
    "    # given features\n",
    "    'prior_question_elapsed_time',\n",
    "    \n",
    "    # other features\n",
    "    \"hmean_user_content_accuracy\",\n",
    "#     \"hmean_user_tags_accuracy\",\n",
    "#     \"hmean_user_part_accuracy\",\n",
    "    'user_content_attempts'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"part\",\n",
    "#     'prior_question_had_explanation',\n",
    "    'tags'\n",
    "]\n",
    "\n",
    "train_data = lgb.Dataset(\n",
    "    data=updated_train_df[features + categorical_features],\n",
    "    label=updated_train_df[target],\n",
    "    categorical_feature=categorical_features,\n",
    "    free_raw_data=False\n",
    ")\n",
    "\n",
    "val_data = lgb.Dataset(\n",
    "    data=updated_val_df[features + categorical_features],\n",
    "    label=updated_val_df[target],\n",
    "    categorical_feature=categorical_features,\n",
    "    free_raw_data=False,\n",
    "    reference=train_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del updated_train_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    "    \"objective\":\"binary\",\n",
    "    \"metric\":\"auc\"\n",
    "}\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "model = None\n",
    "\n",
    "start = time()\n",
    "model = lgb.train(\n",
    "    params = lgbm_params,\n",
    "    train_set = train_data, \n",
    "    valid_sets = [train_data, val_data], \n",
    "    init_model = model,\n",
    "    num_boost_round = 10_000,\n",
    "    verbose_eval = 10,\n",
    "    early_stopping_rounds = 50,\n",
    "    evals_result = evals_result,\n",
    "    categorical_feature = categorical_features\n",
    ")\n",
    "\n",
    "model.save_model('model.txt')\n",
    "\n",
    "print(f'{time() - start:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.feature_importance(\"gain\")\n",
    "\n",
    "feature_importances /= np.sum(feature_importances)\n",
    "\n",
    "for i in range(len(features)):\n",
    "    print(f\"{features[i]}: {feature_importances[i]:.3f}\")\n",
    "  \n",
    "for i in range(len(categorical_features)):\n",
    "    print(f\"{categorical_features[i]}: {feature_importances[len(features)+i]:.3f}\")\n",
    "    \n",
    "lgb.plot_importance(model, importance_type='gain', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.feature_importance(\"split\")\n",
    "\n",
    "for i in range(len(features)):\n",
    "    print(f\"{features[i]}: {feature_importances[i]}\")\n",
    "\n",
    "for i in range(len(categorical_features)):\n",
    "    print(f\"{categorical_features[i]}: {feature_importances[len(features)+i]:.2f}\")\n",
    "\n",
    "lgb.plot_importance(model, importance_type = 'split', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_dtype = {'row_id': 'int64',\n",
    "     'timestamp': 'int64',\n",
    "     'user_id': 'int32',\n",
    "     'content_id': 'int16',\n",
    "     'content_type_id': 'int8',\n",
    "     'prior_question_elapsed_time': 'float32',\n",
    "     'prior_question_had_explanation': 'category'\n",
    "}\n",
    "\n",
    "example_test = pd.read_csv('../input/riiid-test-answer-prediction/example_test.csv')\n",
    "\n",
    "example_test = update_data(example_test, users_state, questions)\n",
    "\n",
    "example_test = example_test.astype(test_dtype)\n",
    "\n",
    "model.predict(example_test[features + categorical_features], num_iteration=model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dtype = {\n",
    "    'row_id': 'int64',\n",
    "    'timestamp': 'int64',\n",
    "    'user_id': 'int32',\n",
    "    'content_id': 'int16',\n",
    "    'content_type_id': 'int8',\n",
    "    'prior_question_elapsed_time': 'float32',\n",
    "#     'prior_question_had_explanation': 'category'\n",
    "}\n",
    "\n",
    "users_state = update_users_state(users_state, updated_val_df)\n",
    "\n",
    "prev_test_df = None\n",
    "\n",
    "for idx, (test_df, _) in tqdm(enumerate(iter_test)):\n",
    "    if prev_test_df is not None:\n",
    "        prev_test_df['answered_correctly'] = eval(test_df['prior_group_answers_correct'].iloc[0])\n",
    "        users_state = update_users_state(users_state, prev_test_df[lambda x:x['content_type_id'] == 0])\n",
    "        \n",
    "        train_data = val_data\n",
    "        \n",
    "        val_data = lgb.Dataset(data=prev_test_df[features+categorical_features],\n",
    "                               label=prev_test_df[target],\n",
    "                               categorical_feature=categorical_features,\n",
    "                               free_raw_data=False,\n",
    "                               reference=train_data\n",
    "                              )\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params = lgbm_params,\n",
    "            train_set = train_data,\n",
    "            valid_sets = [train_data, val_data],\n",
    "            init_model = model,\n",
    "            keep_training_booster=True,\n",
    "            num_boost_round = 10_000,\n",
    "            verbose_eval = 10,\n",
    "            early_stopping_rounds = 50,\n",
    "            categorical_feature = categorical_features\n",
    "            )\n",
    "    \n",
    "    test_df = update_data(test_df, users_state, questions)\n",
    "    \n",
    "    test_df = test_df.astype(test_dtype)\n",
    "\n",
    "    test_df['answered_correctly'] = model.predict(test_df[features + categorical_features], \n",
    "                                                  num_iteration=model.best_iteration)\n",
    "        \n",
    "    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])\n",
    "    \n",
    "    prev_test_df = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
