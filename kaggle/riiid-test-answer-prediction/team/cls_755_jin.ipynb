{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "!pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import datatable as dt\n",
    "import lightgbm as lgb\n",
    "import riiideducation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = riiideducation.make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = dt.fread(\"../input/riiid-test-answer-prediction/train.csv\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['timestamp', 'user_id', 'content_id', 'content_type_id', 'answered_correctly', 'prior_question_elapsed_time', 'prior_question_had_explanation']]\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['content_id'] = train['content_id'].astype('int16')\n",
    "train['content_type_id'] = train['content_type_id'].astype('int8')\n",
    "train['answered_correctly'] = train['answered_correctly'].astype('int8')\n",
    "train['prior_question_elapsed_time'] = train['prior_question_elapsed_time'].astype('float32')\n",
    "train['prior_question_had_explanation'] = train['prior_question_had_explanation'].astype('boolean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_shape = train.shape[0]\n",
    "train = train[train.content_type_id==0]\n",
    "print(f'remove {origin_shape-train.shape[0]} useless datas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_mean = train['prior_question_elapsed_time'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values(['timestamp'], ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_c = train[['content_id','answered_correctly']].groupby(['content_id']).agg(['mean', 'sum', np.size, np.std])\n",
    "results_c.columns = ['correctly_mean_content', 'correctly_sum_content', 'correctly_count_content', 'correctly_std_content']\n",
    "\n",
    "results_u = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean', 'sum', np.size, np.std])\n",
    "results_u.columns = ['correctly_mean_user', 'correctly_sum_user', 'correctly_count_user', 'correctly_std_user']\n",
    "\n",
    "results_etu = train[['user_id','prior_question_elapsed_time']].groupby(['user_id']).agg(['mean'])\n",
    "results_etu.columns = ['elapsed_time_user']\n",
    "\n",
    "results_etc = train[['content_id','prior_question_elapsed_time']].groupby(['content_id']).agg(['mean'])\n",
    "results_etc.columns = ['elapsed_time_content_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['timestamp', 'content_type_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = pd.DataFrame()\n",
    "for i in range(4):\n",
    "    last_records = train.drop_duplicates('user_id', keep='last')\n",
    "    train = train[~train.index.isin(last_records.index)]\n",
    "    validation = validation.append(last_records)\n",
    "    \n",
    "len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame()\n",
    "for i in range(15):\n",
    "    last_records = train.drop_duplicates('user_id', keep='last')\n",
    "    train = train[~train.index.isin(last_records.index)]\n",
    "    X = X.append(last_records)\n",
    "    \n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(X, results_u, on=['user_id'], how=\"left\")\n",
    "X = pd.merge(X, results_c, on=['content_id'], how=\"left\")\n",
    "X = pd.merge(X, results_etu, on=['user_id'], how=\"left\")\n",
    "X = pd.merge(X, results_etc, on=['content_id'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = pd.merge(validation, results_u, on=['user_id'], how=\"left\")\n",
    "validation = pd.merge(validation, results_c, on=['content_id'], how=\"left\")\n",
    "validation = pd.merge(validation, results_etu, on=['user_id'], how=\"left\")\n",
    "validation = pd.merge(validation, results_etc, on=['content_id'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/questions.csv',\n",
    "                            usecols=[0,1,3,4],\n",
    "                            dtype={'question_id': 'int16',\n",
    "                              'part': 'int8','bundle_id': 'int8','tags': 'str'}\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = questions_df[\"tags\"].str.split(\" \", n=10, expand=False)\n",
    "tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags[10033] = []\n",
    "questions_df['tags'] = tags\n",
    "questions_df['tags_count'] = questions_df['tags'].apply(lambda x: len(x))\n",
    "questions_df = questions_df[questions_df['tags_count']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tags_list = []\n",
    "for tag in questions_df['tags'].tolist():\n",
    "    tags_list.extend(tag)\n",
    "tags_counter = dict(Counter(tags_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_appr_means(tags):\n",
    "    l = []\n",
    "    for tag in tags:\n",
    "        l.append(tags_counter[tag])\n",
    "    return np.mean(l)\n",
    "\n",
    "questions_df['tags_appr_mean'] = questions_df['tags'].apply(tag_appr_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df.drop(['tags'], axis=1, inplace=True)\n",
    "questions_df['part'] = questions_df['part'] - 1\n",
    "questions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['prior_question_had_explanation'].fillna(False, inplace=True)\n",
    "X['prior_question_had_explanation'] = X['prior_question_had_explanation'].astype(np.int8)\n",
    "X['prior_question_elapsed_time'].fillna(elapsed_mean, inplace=True)\n",
    "\n",
    "validation['prior_question_had_explanation'].fillna(False, inplace=True)\n",
    "validation['prior_question_had_explanation'] = validation['prior_question_had_explanation'].astype(np.int8)\n",
    "validation['prior_question_elapsed_time'].fillna(elapsed_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(X, questions_df, left_on='content_id', right_on='question_id', how='left')\n",
    "validation = pd.merge(validation, questions_df, left_on='content_id', right_on='question_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_m = pd.merge(train, questions_df, left_on='content_id', right_on='question_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_m_part = train_m[['part','answered_correctly']].groupby(['part']).agg(['mean', 'sum', np.std])\n",
    "train_m_part.columns = ['correctly_mean_part', 'correctly_sum_part', 'correctly_std_part']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(X, train_m_part, on=['part'], how=\"left\")\n",
    "validation = pd.merge(validation, train_m_part, on=['part'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop('bundle_id', axis=1, inplace=True)\n",
    "validation.drop('bundle_id', axis=1, inplace=True)\n",
    "X.drop('question_id', axis=1, inplace=True)\n",
    "validation.drop('question_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.dropna()\n",
    "validation = validation.dropna()\n",
    "\n",
    "X = reduce_mem_usage(X)\n",
    "validation = reduce_mem_usage(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X['answered_correctly']\n",
    "X = X.drop(['answered_correctly'], axis=1)\n",
    "\n",
    "y_val = validation['answered_correctly']\n",
    "X_val = validation.drop(['answered_correctly'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'max_bin': 700,\n",
    "    'learning_rate': 0.05,\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(X, y, categorical_feature=['part', 'prior_question_had_explanation'])\n",
    "lgb_eval = lgb.Dataset(X_val, y_val, categorical_feature=['part', 'prior_question_had_explanation'], reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.train(\n",
    "    params, lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_eval],\n",
    "    verbose_eval=100,\n",
    "    num_boost_round=100000,\n",
    "    early_stopping_rounds=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_features = list(X.columns)\n",
    "\n",
    "#lgb_model = lgb.Booster(model_file='../input/lgb-1019/lgb_10_19.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_pred = model.predict(X_val)\n",
    "y_true = np.array(y_val)\n",
    "roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_model('lgb_1020.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (test_df, sample_prediction_df) in env.iter_test():\n",
    "    test_df = pd.merge(test_df, results_u, on=['user_id'], how=\"left\")\n",
    "    test_df = pd.merge(test_df, results_c, on=['content_id'], how=\"left\")\n",
    "    test_df = pd.merge(test_df, results_etu, on=['user_id'], how=\"left\")\n",
    "    test_df = pd.merge(test_df, results_etc, on=['content_id'], how=\"left\")\n",
    "    test_df = pd.merge(test_df, questions_df, left_on='content_id', right_on='question_id', how='left')\n",
    "    test_df = pd.merge(test_df, train_m_part, on=['part'], how=\"left\")\n",
    "    test_df['correctly_mean_content'].fillna(0.5, inplace=True)\n",
    "    test_df['correctly_mean_user'].fillna(0.5, inplace=True)\n",
    "    test_df['correctly_mean_part'].fillna(0.5, inplace=True)\n",
    "    test_df['part'] = test_df['part'] - 1\n",
    "\n",
    "    test_df['part'].fillna(4, inplace=True)\n",
    "    test_df['prior_question_elapsed_time'].fillna(elapsed_mean, inplace=True)\n",
    "    test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n",
    "    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].astype(np.int8)\n",
    "    \n",
    "    test_df.fillna(0, inplace=True)\n",
    "    \n",
    "    test_data = test_df[columns_features]\n",
    "    test_df['answered_correctly'] = model.predict(test_data)\n",
    "    env.predict(test_df.loc[test_df['content_type_id']==0, ['row_id', 'answered_correctly']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
