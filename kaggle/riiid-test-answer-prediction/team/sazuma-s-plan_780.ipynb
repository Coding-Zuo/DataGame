{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm.notebook import tqdm\n",
    "import lightgbm as lgb\n",
    "import datatable as dt\n",
    "from category_encoders import TargetEncoder\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types_dict = {\n",
    "    'timestamp': 'int64',\n",
    "    'user_id': 'int32', \n",
    "    'content_id': 'int16', \n",
    "    'content_type_id': 'int8',\n",
    "    'task_container_id': 'int16',\n",
    "    'answered_correctly': 'int8',\n",
    "    'user_answer': 'int8',\n",
    "    'prior_question_elapsed_time': 'float32', \n",
    "    'prior_question_had_explanation': 'bool'\n",
    "}\n",
    "\n",
    "train_size = 10\n",
    "valid_size = 2\n",
    "target = 'answered_correctly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_mem():\n",
    "    %reset -f out\n",
    "    %reset -f in\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_df = dt.fread('../input/riiid-test-answer-prediction/train.csv', columns=set(data_types_dict.keys())).to_pandas()\n",
    "#train_df = train_df[train_df[target] != -1].reset_index(drop=True)\n",
    "train_df['prior_question_had_explanation'].fillna(False, inplace=True)\n",
    "train_df = train_df.astype(data_types_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_watch_lecture_agg = train_df.groupby('user_id')['content_type_id'].agg(['count', 'sum'])\n",
    "\n",
    "train_df = train_df[train_df[target] != -1].reset_index(drop=True)\n",
    "prior_question_elapsed_time_avg = train_df['prior_question_elapsed_time'].dropna().values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agg = train_df.groupby('user_id')[target].agg(['sum', 'count'])\n",
    "content_agg = train_df.groupby('content_id')[target].agg(['sum', 'count', 'std', 'skew'])\n",
    "time_content_agg = train_df.groupby('content_id')['prior_question_elapsed_time'].agg(['max', 'min', 'std', 'skew'])\n",
    "user_timestamp_agg = train_df.groupby('user_id')['timestamp'].tail()\n",
    "task_container_agg = train_df.groupby('user_id')['task_container_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df[\"attempt_no\"] = 1\n",
    "#train_df[\"attempt_one_question\"] = train_df[['user_id','content_id','attempt_no']].groupby([\"user_id\",\"content_id\"])[\"attempt_no\"].cumsum()\n",
    "#train_df.drop(\"attempt_no\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt_one_dict = train_df[['user_id', 'content_id','attempt_one_question']].groupby(['user_id','content_id'])['attempt_one_question'].max().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.groupby('user_id').tail(train_size).reset_index(drop=True)\n",
    "print(train_df.shape)\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_csv(\n",
    "    '../input/riiid-test-answer-prediction/questions.csv', \n",
    "    usecols=[0, 1, 2, 3, 4],\n",
    "    dtype={'question_id': 'int16', 'bundle_id': 'int16', \n",
    "           'correct_answer': 'int8', 'part': 'int8', 'tags': 'str'}\n",
    ")\n",
    "\n",
    "lecture = pd.read_csv(\n",
    "    '../input/riiid-test-answer-prediction/lectures.csv',\n",
    "    usecols=[0, 1, 2, 3],\n",
    "    dtype={'lecture_id': 'int16', 'tag': 'int16', 'part': 'int8', 'type_of': 'str'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tags_set = set(list(lecture['tag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_type_of(x):\n",
    "    types = []\n",
    "    for i in x:\n",
    "        if i in set(lecture['tag'].tolist()):\n",
    "            des = list(lecture.loc[lecture['tag']==i, 'type_of'])\n",
    "            types.extend(des)\n",
    "        else:\n",
    "            continue\n",
    "    return types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = questions[\"tags\"].str.split(\" \", n=10, expand=False)\n",
    "tags[10033] = ['162']\n",
    "questions['tags'] = tags\n",
    "\n",
    "def str_to_int(x):\n",
    "    result = []\n",
    "    for i in x:\n",
    "        result.append(int(i))\n",
    "    return set(result)\n",
    "\n",
    "questions['tags'] = questions['tags'].apply(str_to_int)\n",
    "\n",
    "questions['question_type_of'] = questions['tags'].apply(question_type_of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions['type_of_solving_question'] = questions['question_type_of'].apply(lambda x: x.count('solving question'))\n",
    "questions['type_of_concept'] = questions['question_type_of'].apply(lambda x: x.count('concept'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions['tags_count'] = questions['tags'].apply(lambda x: len(x))\n",
    "#questions_df = questions_df[questions_df['tags_count']!=0]\n",
    "\n",
    "tags_list = []\n",
    "for tag in questions['tags'].tolist():\n",
    "    tags_list.extend(list(tag))\n",
    "tags_counter = dict(Counter(tags_list))\n",
    "\n",
    "bundle_dict = questions['bundle_id'].value_counts().to_dict()\n",
    "questions['bundle_size'] = questions['bundle_id'].apply(lambda x: bundle_dict[x])\n",
    "\n",
    "def tag_appr_means(tags):\n",
    "    l = []\n",
    "    for tag in tags:\n",
    "        l.append(tags_counter[tag])\n",
    "    return np.mean(l)\n",
    "\n",
    "def tag_appr_sum(tags):\n",
    "    l = []\n",
    "    for tag in tags:\n",
    "        l.append(tags_counter[tag])\n",
    "    return np.sum(l)\n",
    "\n",
    "def tag_appr_most(tags):\n",
    "    tag_max = 0\n",
    "    for tag in tags:\n",
    "        if tags_counter[tag] > tag_max:\n",
    "            tag_max = tags_counter[tag]\n",
    "        else:\n",
    "            continue\n",
    "    return tag_max\n",
    "\n",
    "questions['tags_appr_mean'] = questions['tags'].apply(tag_appr_means)\n",
    "questions['tags_appr_sum'] = questions['tags'].apply(tag_appr_sum)\n",
    "questions['tags_appr_most'] = questions['tags'].apply(tag_appr_most)\n",
    "\n",
    "#questions.loc[questions['question_id']==10033, 'tags_count'] = questions['tags_count'].mode()[0]\n",
    "#questions.loc[questions['question_id']==10033, 'tags_appr_mean'] = questions['tags_appr_mean'].median()\n",
    "#questions.loc[questions['question_id']==10033, 'type_of_concept'] = 1\n",
    "#questions.loc[questions['question_id']==10033, 'type_of_solving_question'] = 1\n",
    "#questions.loc[questions['question_id']==10033, 'type_of_starter'] = 0\n",
    "#questions.loc[questions['question_id']==10033, 'type_of_intention'] = 0\n",
    "\n",
    "#questions_df.drop(['tags', 'question_type_of'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.set_index('question_id', inplace=True)\n",
    "train_df = train_df.join(questions, on=['content_id'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['only_tag'].mode()\n",
    "# {73}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def extract_only_tag(x):\n",
    "#    return tags_set.intersection(x)\n",
    "\n",
    "#train_df['only_tag'] = train_df['tags'].apply(extract_only_tag)\n",
    "#train_df['only_tag'] = train_df['only_tag'].apply(lambda x: list(x)[0] if len(x)>0 else 73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enc = TargetEncoder(cols=['only_tag'], return_df=False)\n",
    "#enc.fit(train_df['only_tag'], train_df[target])\n",
    "del train_df\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feats = ['prior_question_elapsed_time', 'prior_question_had_explanation', 'user_avg',\n",
    "         'attempt_one_question', 'elapsed_time_std', 'elapsed_time_skew', 'max-elapsed_time', \n",
    "         'elapsed_time-min', 'part', 'type_of_solving_question', 'type_of_concept', \n",
    "         'tags_count', 'bundle_size', 'tags_appr_mean', 'tags_appr_sum', 'is_162', 'tag_alone', \n",
    "         'content_count', 'content_avg', 'content_std', 'content_skew', 'hmean_by_user_content'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['prior_question_elapsed_time', 'prior_question_had_explanation', \n",
    "         'watch_lecture_%',\n",
    "         'watch_lecture_sum', \n",
    "         'user_avg', \n",
    "         'lag_time',\n",
    "         'task_container_id_nunique',\n",
    "         'attempt_one_question', \n",
    "         'elapsed_time_std', 'elapsed_time_skew', 'max-elapsed_time', \n",
    "         'elapsed_time-min', 'part', 'type_of_solving_question', 'type_of_concept', \n",
    "         'tags_count', 'bundle_size', 'tags_appr_mean', 'tags_appr_sum', \n",
    "         #'only_tag',\n",
    "         #'is_162', 'tag_alone', \n",
    "         'content_count', 'content_avg', 'content_std', 'content_skew', \n",
    "         'hmean_by_user_content'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['prior_question_had_explanation', 'is_162', 'tag_alone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_model('sazuma_tail_24_6.txt')\n",
    "model = lgb.Booster(model_file='../input/riiids-models-and-dicts/sazuma_tail_1927_7.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_attempt_one(user_id, content_id):\n",
    "    k = (user_id, content_id)\n",
    "\n",
    "    if k in attempt_one_dict.keys():\n",
    "        attempt_one_dict[k] += 1\n",
    "        return attempt_one_dict[k]\n",
    "\n",
    "    attempt_one_dict[k] = 1\n",
    "    return attempt_one_dict[k]\n",
    "\n",
    "def mapper_162(x):\n",
    "    if '162' in x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def mapper_tag_alone(x):\n",
    "    if  len(x)==1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt_one_dict = np.load('../input/riiids-models-and-dicts/attempt_one_dict_1927_7.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sum_dict = user_agg['sum'].astype('int16').to_dict(defaultdict(int))\n",
    "user_count_dict = user_agg['count'].astype('int16').to_dict(defaultdict(int))\n",
    "content_sum_dict = content_agg['sum'].astype('int32').to_dict(defaultdict(int))\n",
    "content_count_dict = content_agg['count'].astype('int32').to_dict(defaultdict(int))\n",
    "user_watch_lecture_sum_dict = user_watch_lecture_agg['sum'].astype('int16').to_dict(defaultdict(int))\n",
    "user_watch_lecture_count_dict = user_watch_lecture_agg['count'].astype('int16').to_dict(defaultdict(int))\n",
    "user_timestamp_dict = user_timestamp_agg.to_dict(defaultdict(int))\n",
    "task_container_dict = task_container_agg.to_dict(defaultdict(int))\n",
    "\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import riiideducation\n",
    "env = riiideducation.make_env()\n",
    "iter_test = env.iter_test()\n",
    "prior_test_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    if prior_test_df is not None:\n",
    "        prior_test_df[target] = eval(test_df['prior_group_answers_correct'].iloc[0])\n",
    "        \n",
    "        user_ids = prior_test_df['user_id'].values\n",
    "        content_type_ids = prior_test_df['content_type_id'].values\n",
    "        \n",
    "        for user_id, content_type_id in zip(user_ids, content_type_ids):\n",
    "            user_watch_lecture_sum_dict[user_id] += content_type_id\n",
    "            user_watch_lecture_count_dict[user_id] += 1\n",
    "        \n",
    "        prior_test_df = prior_test_df[prior_test_df[target] != -1].reset_index(drop=True)\n",
    "        \n",
    "        user_ids = prior_test_df['user_id'].values\n",
    "        content_ids = prior_test_df['content_id'].values\n",
    "        targets = prior_test_df[target].values\n",
    "        \n",
    "        for user_id, content_id, answered_correctly in zip(user_ids, content_ids, targets):\n",
    "            user_sum_dict[user_id] += answered_correctly\n",
    "            user_count_dict[user_id] += 1\n",
    "            content_sum_dict[content_id] += answered_correctly\n",
    "            content_count_dict[content_id] += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    prior_test_df = test_df.copy()\n",
    "    \n",
    "    test_df = test_df.sort_values(by='timestamp')\n",
    "    \n",
    "    user_task_container_shift = test_df.groupby('user_id')['task_container_id'].shift()\n",
    "    test_df['task_container_id_shift'] = user_task_container_shift\n",
    "    test_df['task_container_id_not_equal'] = (test_df['task_container_id'] != test_df['task_container_id_shift']).astype(np.int8)\n",
    "    \n",
    "    user_watch_lecture_sum = np.zeros(len(test_df), dtype=np.int16)\n",
    "    user_watch_lecture_count = np.zeros(len(test_df), dtype=np.int16)\n",
    "    user_task_container_id_count = np.zeros(len(test_df), dtype=np.int32)\n",
    "    \n",
    "    for i, (user_id, content_type_id, task_container_id_not_equal) in enumerate(zip(test_df['user_id'].values, test_df['content_type_id'].values, test_df['task_container_id_not_equal'].values)):\n",
    "        user_watch_lecture_sum[i] = user_watch_lecture_sum_dict[user_id]\n",
    "        user_watch_lecture_count[i] = user_watch_lecture_count_dict[user_id]\n",
    "        # update the numbers of task_container per user\n",
    "        task_container_dict[user_id] = task_container_dict[user_id] + task_container_id_not_equal\n",
    "        user_task_container_id_count[i] = task_container_dict[user_id]\n",
    "    \n",
    "    test_df['watch_lecture_%'] = user_watch_lecture_sum / user_watch_lecture_count\n",
    "    test_df['watch_lecture_sum'] = user_watch_lecture_sum\n",
    "    test_df['task_container_id_nunique'] = user_task_container_id_count\n",
    "    \n",
    "    test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop=True)\n",
    "    \n",
    "    test_df = test_df.join(questions, on='content_id', how='left')\n",
    "    \n",
    "    test_df['is_162'] = test_df['tags'].apply(mapper_162)\n",
    "    test_df['tag_alone'] = test_df['tags'].apply(mapper_tag_alone)\n",
    "    \n",
    "    user_sum = np.zeros(len(test_df), dtype=np.int16)\n",
    "    user_count = np.zeros(len(test_df), dtype=np.int16)\n",
    "    content_sum = np.zeros(len(test_df), dtype=np.int32)\n",
    "    content_count = np.zeros(len(test_df), dtype=np.int32)\n",
    "    user_timestamp_diffs = np.zeros(len(test_df), dtype=np.int64)\n",
    "    \n",
    "    for i, (user_id, timestamp, content_id) in enumerate(zip(test_df['user_id'].values, test_df['timestamp'].values, test_df['content_id'].values)):\n",
    "        user_sum[i] = user_sum_dict[user_id]\n",
    "        user_count[i] = user_count_dict[user_id]\n",
    "        content_sum[i] = content_sum_dict[content_id]\n",
    "        content_count[i] = content_count_dict[content_id]\n",
    "        ### every user's timestamp diff ###\n",
    "        user_timestamp_diffs[i] = timestamp - user_timestamp_dict.get(user_id, 0)\n",
    "        user_timestamp_dict[user_id] = timestamp\n",
    "        \n",
    "    #test_df['only_tag'] = test_df['tags'].apply(extract_only_tag)\n",
    "    #test_df['only_tag'] = test_df['only_tag'].apply(lambda x: list(x)[0] if len(x)>0 else 73)\n",
    "    #test_df['only_tag'] = enc.transform(test_df['only_tag'])\n",
    "    \n",
    "    test_df['lag'] = user_timestamp_diffs\n",
    "    test_df['lag'] = test_df['lag'].replace(0, np.nan)\n",
    "    test_df['lag'] = test_df['lag'].fillna(method='ffill')\n",
    "    test_df['lag_time'] = test_df['lag'] - test_df['prior_question_elapsed_time']\n",
    "    test_df['user_avg'] = user_sum / user_count\n",
    "    #test_df['user_count'] = user_count\n",
    "    #test_df['user_sum'] = user_sum\n",
    "    test_df['content_count'] = content_count\n",
    "    test_df['content_avg'] = content_sum / content_count\n",
    "    test_df['content_std'] = test_df['content_id'].map(content_agg['std'])\n",
    "    test_df['content_skew'] = test_df['content_id'].map(content_agg['skew'])\n",
    "    test_df['hmean_by_user_content'] = 2*test_df['user_avg']*test_df['content_avg'] / (test_df['user_avg']+test_df['content_avg'])\n",
    "    \n",
    "    test_df['elapsed_time_std'] = test_df['content_id'].map(time_content_agg['std'])\n",
    "    test_df['elapsed_time_skew'] = test_df['content_id'].map(time_content_agg['skew'])\n",
    "    test_df['max-elapsed_time'] = test_df['content_id'].map(time_content_agg['max']) - test_df['prior_question_elapsed_time']\n",
    "    test_df['elapsed_time-min'] = test_df['prior_question_elapsed_time'] - test_df['content_id'].map(time_content_agg['min'])\n",
    "    \n",
    "    test_df[\"attempt_one_question\"] = test_df[[\"user_id\", \"content_id\"]].apply(lambda row: get_max_attempt_one(row[\"user_id\"], row[\"content_id\"]), axis=1)\n",
    "    test_df[\"attempt_one_question\"] = test_df[\"attempt_one_question\"].apply(lambda x: 3 if x>3 else x)\n",
    "    \n",
    "    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(False).astype('bool')\n",
    "    test_df['prior_question_elapsed_time'] = test_df['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_avg)\n",
    "    \n",
    "    test_df[target] = model.predict(test_df[feats])\n",
    "    env.predict(test_df[['row_id', target]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
